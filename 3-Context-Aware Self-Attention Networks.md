# Context-Aware Self-Attention Networks

source: https://arxiv.org/abs/1902.05766

### Introduction
This paper proposes a new type of self-attention mechanism that takes into account the context of the input sequence. The proposed model, called the Context-Aware Self-Attention Network (CASA), is designed to capture the contextual information of a sequence by attending to different parts of the sequence based on their relevance to the context. They avaluate it several natural language processing tasks, including sentiment analysis and machine translation, and demonstra models in terms of accuracy and efficiency.


| Command | Description |
| --- | --- |
|Topic| Context-Aware Self-Attention Networks. |
|Problem| Standard self-attention mechanisms may not effectively capture contextual information, leading to reduced performance on certain natural language processing tasks.   |
|Related Work| Existing work in self-attention mechanisms and contextual modeling, such as the Transformer model and the ELMo model.|
